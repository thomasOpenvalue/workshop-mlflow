{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop MLflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__*Objectives*__\n",
    "\n",
    "Use **ML Flow Tracking** to follow a model key metrics\n",
    "\n",
    "__*Steps*__\n",
    "\n",
    "1. Import Libs\n",
    "2. Load cleaned data\n",
    "    * Load CSV\n",
    "    * Short Data description\n",
    "3. Understand the data\n",
    "    * Pandas Profiling\n",
    "4. Machine Learning\n",
    "    * Preprocess\n",
    "    * Metrics\n",
    "    * Models\n",
    "    * Results\n",
    "5. MLFlow\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trick to avoid warning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T06:48:53.403325Z",
     "start_time": "2019-04-09T06:48:53.399643Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas_profiling as pdp\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T06:20:48.484408Z",
     "start_time": "2019-04-09T06:20:48.481622Z"
    }
   },
   "outputs": [],
   "source": [
    "# Display all the columns of Pandas Dataframe\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T06:22:34.137997Z",
     "start_time": "2019-04-09T06:22:33.960217Z"
    }
   },
   "outputs": [],
   "source": [
    "df_all = pd.read_csv('./data/energydata_complete.csv')\n",
    "\n",
    "print('nb observations: {} - nb features: {}'.format(*df_all.shape))\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information about the data set\n",
    "\n",
    "Column Name  | Description | Unit\n",
    "------------ | ----------- | -----------\n",
    "date | year-month-day hour:minute:second | \n",
    "lights | energy use of light fixtures the house | Wh \n",
    "T1 | Temperature in kitchen area, | Celsius \n",
    "RH_1 | Humidity in kitchen area, | % \n",
    "T2 | Temperature in living room area, | Celsius \n",
    "RH_2 | Humidity in living room area, | % \n",
    "T3 | Temperature in laundry room area \n",
    "RH_3 | Humidity in laundry room area, | % \n",
    "T4 | Temperature in office room, | Celsius \n",
    "RH_4 | Humidity in office room, | % \n",
    "T5 | Temperature in bathroom, | Celsius \n",
    "RH_5 | Humidity in bathroom, | % \n",
    "T6 | Temperature outside the building (north side), | Celsius \n",
    "RH_6 | Humidity outside the building (north side), | % \n",
    "T7 | Temperature in ironing room , | Celsius \n",
    "RH_7 | Humidity in ironing room, | % \n",
    "T8 | Temperature in teenager room 2, | Celsius \n",
    "RH_8 | Humidity in teenager room 2, | % \n",
    "T9 | Temperature in parents room, | Celsius \n",
    "RH_9 | Humidity in parents room, | % \n",
    "To | Temperature outside (from Chievres weather station), | Celsius \n",
    "Pressure | (from Chievres weather station), | mm Hg \n",
    "RH_out | Humidity outside (from Chievres weather station), | % \n",
    "Wind speed | (from Chievres weather station), | m/s \n",
    "Visibility | (from Chievres weather station), | km \n",
    "Tdewpoint | (from Chievres weather station), Â°C \n",
    "rv1 | Random variable 1, nondimensional \n",
    "rv2 | Random variable 2, nondimensional \n",
    "------------ | ----------- | -----------\n",
    "Appliances | energy use | Wh\n",
    "\n",
    "\n",
    "We will create a report named `report-all-data.html` in the repo `./analysis`.\n",
    "This report helps us to understand all distribution and correlation in the data set. You can go into that repo and open it in your browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T06:33:19.056094Z",
     "start_time": "2019-04-09T06:33:19.039473Z"
    }
   },
   "outputs": [],
   "source": [
    "# Just Random variable for robustness\n",
    "df_all.drop(columns=['date', 'rv1', 'rv2'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get report analysis : [PandasProfiling](https://github.com/pandas-profiling/pandas-profiling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T06:55:18.315576Z",
     "start_time": "2019-04-09T06:55:18.312182Z"
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\"./analysis\"):\n",
    "    os.mkdir(\"./analysis\") # Create repo because does not exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T06:55:25.378775Z",
     "start_time": "2019-04-09T06:55:21.706428Z"
    }
   },
   "outputs": [],
   "source": [
    "profile = pdp.ProfileReport(df_all)\n",
    "profile.to_file(outputfile=\"./analysis/report-all-data.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the created [report](./analysis/report-all-data.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's talk about Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__*What is the objective of the model?*__\n",
    "\n",
    "=> Predict the Quantity of Energy used\n",
    "\n",
    "We will use a first ML model to see what kind of information we need to record to (for example) evaluate the capacity of the model, if we suffer from overfitting or underfitting etc. From that we will understand why `mlflow` is a great tool for tracking metrics and save artifacts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T07:23:17.162199Z",
     "start_time": "2019-04-09T07:23:17.159463Z"
    }
   },
   "source": [
    "## Metrics to evaluate the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T07:24:40.874216Z",
     "start_time": "2019-04-09T07:24:40.867439Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, make_scorer\n",
    "\n",
    "def eval_metrics(actual, pred):\n",
    "    rmse = np.sqrt(mean_squared_error(actual, pred))\n",
    "    mae = mean_absolute_error(actual, pred)\n",
    "    r2 = r2_score(actual, pred)\n",
    "    return rmse, mae, r2\n",
    "\n",
    "\n",
    "def cv_eval_metrics(cv_model):\n",
    "    rmse = np.sqrt(np.mean(cv_model['test_mse']))\n",
    "    mae = np.mean(cv_model['test_mae'])\n",
    "    r2 = np.mean(cv_model['test_r2'])\n",
    "    return rmse, mae, r2\n",
    "\n",
    "\n",
    "def scatter_plot_result(y_actual, y_pred, name):\n",
    "    plt.scatter(y_actual, y_pred)\n",
    "    plt.ylabel('Target predicted')\n",
    "    plt.xlabel('True Target')\n",
    "    plt.title(model_name)\n",
    "    \n",
    "    pos_x = y_actual.max() * 0.60\n",
    "    pos_y = y_pred.max() * 0.90\n",
    "    \n",
    "    plt.text(pos_x, pos_y, r'$RMSE=%.2f, R^2$=%.2f, MAE=%.2f' % (np.sqrt(mean_squared_error(y_actual, y_pred)), \n",
    "                                              r2_score(y_actual, y_pred), \n",
    "                                              mean_absolute_error(y_actual, y_pred)))\n",
    "    plt.plot([0, y_actual.max()], [0, y_actual.max()], ls=\"--\", c=\".3\")\n",
    "    plt.savefig('./scatter_results-{}.png'.format(name)) # Save to be included in Artefacts\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def plot_cv_results(iMetric, iResCV, model_name=None):\n",
    "    # multiple line plot\n",
    "    plt.plot(range(1,6)\n",
    "             , iResCV[\"test_\" + iMetric]\n",
    "             , marker='o'\n",
    "             , markerfacecolor='blue'\n",
    "             , markersize=12, color='skyblue'\n",
    "             , linewidth=4)\n",
    "    plt.plot(range(1,6)\n",
    "             , iResCV[\"train_\" + iMetric]\n",
    "             , marker='o'\n",
    "             , markerfacecolor='red'\n",
    "             , markersize=12\n",
    "             , color='red'\n",
    "             , linewidth=4)\n",
    "    plt.xticks(range(1,6))\n",
    "    plt.legend(('test','train'))\n",
    "    plt.title(iMetric)\n",
    "    if model_name is not None:\n",
    "        plt.savefig('./cross_val_results-{}.png'.format(model_name))\n",
    "    plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T07:21:30.170761Z",
     "start_time": "2019-04-09T07:21:30.142566Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_validate, cross_val_predict\n",
    "\n",
    "target_column = \"Appliances\" # \"y\"\n",
    "\n",
    "# Create the scorings dict\n",
    "scorings = {'mse' : make_scorer(mean_squared_error),\n",
    "            \"mae\" : make_scorer(mean_absolute_error),\n",
    "            \"r2\" : make_scorer(r2_score)}\n",
    "\n",
    "# Split features and targets\n",
    "y = df_all[target_column]\n",
    "X = df_all.drop([target_column], axis=1)\n",
    "\n",
    "# Nb Cross Val\n",
    "CV = 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build our first model : [Random Forest Regressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T07:25:17.318585Z",
     "start_time": "2019-04-09T07:24:48.923245Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Instantiate model\n",
    "rfr = RandomForestRegressor(random_state=0, n_estimators=100)\n",
    "\n",
    "# Train model\n",
    "model = cross_validate(rfr, X, y, scoring=scorings, cv=CV, n_jobs=-1, return_train_score=True)\n",
    "\n",
    "print('test rmse: {} - test mae: {} - test r2: {}'.format(*cv_eval_metrics(model)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the cross val results\n",
    "metric_to_plot = \"r2\"\n",
    "plot_cv_results(metric_to_plot, model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think about this model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction\n",
    "y_pred = cross_val_predict(rfr, X, y, cv=CV, n_jobs=-1, verbose=1)\n",
    "scatter_plot_result(y, y_pred, 'RandomForest')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> `Retrain your model with another set of parameters and compare results`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a second model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often, we use a second model in order to challenge the first one..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T07:26:31.035815Z",
     "start_time": "2019-04-09T07:26:30.402044Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.preprocessing import QuantileTransformer, quantile_transform\n",
    "\n",
    "# Instantiate model\n",
    "en = ElasticNet(random_state=0, alpha=0.5, l1_ratio=0.2)\n",
    "\n",
    "# Train model\n",
    "model = cross_validate(en, X, y, scoring=scorings, cv=CV, n_jobs=-1, return_train_score=True)\n",
    "print('test rmse: {} - test mae: {} - test r2: {}'.format(*cv_eval_metrics(model)))\n",
    "\n",
    "# Plot Cross val\n",
    "metric_to_plot = \"r2\"\n",
    "plot_cv_results(metric_to_plot, model)\n",
    "\n",
    "# Make prediction\n",
    "y_pred = cross_val_predict(en, X, y, cv=CV, n_jobs=-1, verbose=1)\n",
    "scatter_plot_result(y, y_pred, 'ElasticNet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> `Retrain your model with another set of parameters and compare results`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optionnel : Understand your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [SHAP](https://github.com/slundberg/shap)\n",
    "A unified approach to explain the output of any machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "# load JS visualization code to notebook\n",
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explain the model's predictions using SHAP values\n",
    "# (same syntax works for LightGBM, CatBoost, and scikit-learn models)\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(train_x, approximate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize the effects of all the features\n",
    "shap.summary_plot(shap_values, train_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility of ML Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point you may want to draw more visualizations to compare your models :\n",
    "    * performance, \n",
    "    * feature importance\n",
    "    * other metrics\n",
    "\n",
    "You understand that we will have do this process **EVERY TIME**, to compare or analyse any model or ML code. \n",
    "\n",
    "Also, **if your data change**, your metrics can change. It would be great to have the history of the data ATTACHED to the code's history\n",
    "\n",
    "__*This is where Tracking with MLflow is useful*__\n",
    "\n",
    "Same exercise in `train.py`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - MLflow [Tracking](https://mlflow.org/docs/latest/tracking.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import TransformedTargetRegressor\n",
    "\n",
    "def get_train_test_data(df):\n",
    "    target_column = \"Appliances\"\n",
    "    # Split data\n",
    "    train, test = train_test_split(df)\n",
    "\n",
    "    train_x = train.drop([target_column], axis=1)\n",
    "    test_x = test.drop([target_column], axis=1)\n",
    "    train_y = train[target_column]\n",
    "    test_y = test[target_column]\n",
    "\n",
    "    return train_x, train_y, test_x, test_y\n",
    "\n",
    "\n",
    "# If you wish to try on classification problem\n",
    "def log_metrics_classification(y_true, y_prediction):\n",
    "    report = classification_report(y_true, y_prediction, output_dict=True)\n",
    "    for class_ in ['0', '1']:\n",
    "        for metric in report[class_]:\n",
    "            log_name = class_ + '_' + metric\n",
    "            # log metrics with log_name ~ 1 line\n",
    "         \n",
    "        \n",
    "def log_metrics_regression(cv_model):\n",
    "    rmse, mae, r2 = cv_eval_metrics(cv_model)\n",
    "    # log metrics here ~ 3 lines\n",
    "\n",
    "def set_mlfow_experiment(experiment_name):\n",
    "    experiment_name = 'Default' if experiment_name is None else experiment_name\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "\n",
    "\n",
    "def run_experiment_elasticnet(df, alpha, l1_ratio, experiment_name=None):\n",
    "    # set exeperiment here ~ 1 line\n",
    "    set_mlfow_experiment(experiment_name)\n",
    "        \n",
    "    # Split features and targets\n",
    "    target_column = \"Appliances\"\n",
    "    y = df[target_column]\n",
    "    X = df.drop([target_column], axis=1)\n",
    "\n",
    "    # Nb Cross Val\n",
    "    CV = 5\n",
    "    \n",
    "    scorings = {'mse' : make_scorer(mean_squared_error),\n",
    "            \"mae\" : make_scorer(mean_absolute_error),\n",
    "            \"r2\" : make_scorer(r2_score)}\n",
    "\n",
    "    with mlflow.start_run():\n",
    "        print(\"Running with alpha: {} - l1_ratio: {}\".format(alpha, l1_ratio))\n",
    "\n",
    "        # fit models\n",
    "        lr = ElasticNet(random_state=0, alpha=alpha, l1_ratio=l1_ratio)\n",
    "        model = cross_validate(lr, X, y, scoring=scorings, cv=CV, n_jobs=-1, return_train_score=True)\n",
    "\n",
    "        prediction_test = cross_val_predict(lr, X, y, cv=CV, n_jobs=-1, verbose=1)\n",
    "\n",
    "        # log parameters\n",
    "        # Your code here ~ 2 lines\n",
    "\n",
    "        # log plots as artifacts\n",
    "        scatter_name = 'ElasticNet'\n",
    "        # save scatter plot as artifact here ~ 2 lines (1 to create the file, 1 to save as artifact)\n",
    "        \n",
    "        # save cv_results ~ 1 line\n",
    "        for metric in ['r2', 'mse', 'mae']:\n",
    "            cv_result_name = metric + '_cv_result_ElasticNet'\n",
    "            # plot cv result here using cv_result_name to save the plot ~ 2 line\n",
    "\n",
    "        # log metrics \n",
    "        log_metrics_regression(model)\n",
    "\n",
    "        # log sklearn model\n",
    "        train_x, train_y, test_x, test_y = get_train_test_data(df)\n",
    "        lr.fit(train_x, train_y)\n",
    "        # log the sklearn model here  ~ 1 line\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# play yourself with parameters\n",
    "# ! both parameters have min 0 and max 1 ! \n",
    "\n",
    "\n",
    "# Remove break to see all runs\n",
    "for alpha in np.arange(0.1, 1, 0.2):\n",
    "    for l1_ratio in np.arange(0.1, 1, 0.2):\n",
    "        run_experiment_elasticnet(df_all, alpha, l1_ratio)\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - MLflow [Projects](https://mlflow.org/docs/latest/projects.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we will \"package\" our project as an MLflow project:**\n",
    "* MLflow Projects are just a convention for organizing and describing your code\n",
    "* Each project is simply a directory of files, or a Git repository, containing your code\n",
    "* MLflow can run some projects based on a convention for placing files in this directory but you can describe your project in more detail by adding a `MLproject file, which is a YAML formatted text` file\n",
    "\n",
    "**We will make our `MLproject file` and define:**\n",
    "* Name\n",
    "* Entry point (you can define several entry point : etl -> train -> test, but here we just have the main entry point)\n",
    "* (Environment is optional, see documentation for more information)\n",
    "\n",
    "\n",
    "-> open it: [MLproject file](./MLproject)\n",
    "\n",
    "\n",
    "**Once you finished your MLproject file**\n",
    "The following command runs your project by read your MLproject file on your local system.\n",
    "\n",
    "```bash \n",
    "~$ mlflow run .\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "**What if you want to change your code, push it on a remote repo and test it with its git hash ?**\n",
    "```bash \n",
    "~$ mlflow run git@github.com:thomasOpenvalue/workshop-mlflow.git --version=84295be...\n",
    "```\n",
    "\n",
    "**more details**:\n",
    "```bash\n",
    "~$ mlflow run --help\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - MLflow [Models](https://mlflow.org/docs/latest/models.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During your runs, you saved/logged your models. Each model is attached to a run and you can see it in your MLflow UI by clicking on a run.\n",
    "\n",
    "You can re-use a given model or serve it. MLflow allows you to use a lot of model type like sagemaker models, sklearn models, keras models.\n",
    "Also you can use the Model API as follow.\n",
    "\n",
    "You we'll need to choose a **path** (if you used mlflow.save_model -> the same used path. If you used mlflow.log_model -> the name you gave for the log) and the **run_id** (choose a run in your UI)\n",
    "\n",
    "for more details : [MLflow Model API](https://mlflow.org/docs/latest/models.html#model-api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow.sklearn\n",
    "\n",
    "sk_model = mlflow.sklearn.load_model(path=\"elastic_net\", run_id=\"2cb....\")\n",
    "\n",
    "pred_test = sk_model.predict(test_x)\n",
    "\n",
    "print('rmse: {} - mae: {} - r2: {}'.format(*eval_metrics(test_y, pred_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
