{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fontaine/anaconda3/envs/workshop-mlflow/lib/python3.7/site-packages/mlflow/utils/environment.py:26: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  env = yaml.load(_conda_header)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_profiling as pdp\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb observations: 19735 - nb features: 29\n"
     ]
    }
   ],
   "source": [
    "df_all = pd.read_csv('./data/energydata_complete.csv')\n",
    "\n",
    "print('nb observations: {} - nb features: {}'.format(*df_all.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information about the data set :\n",
    "1. date time year-month-day hour:minute:second\n",
    "2. lights, energy use of light fixtures in the house in Wh \n",
    "3. T1, Temperature in kitchen area, in Celsius \n",
    "4. RH_1, Humidity in kitchen area, in % \n",
    "5. T2, Temperature in living room area, in Celsius \n",
    "6. RH_2, Humidity in living room area, in % \n",
    "7. T3, Temperature in laundry room area \n",
    "8. RH_3, Humidity in laundry room area, in % \n",
    "9. T4, Temperature in office room, in Celsius \n",
    "10. RH_4, Humidity in office room, in % \n",
    "11. T5, Temperature in bathroom, in Celsius \n",
    "12. RH_5, Humidity in bathroom, in % \n",
    "13. T6, Temperature outside the building (north side), in Celsius \n",
    "14. RH_6, Humidity outside the building (north side), in % \n",
    "15. T7, Temperature in ironing room , in Celsius \n",
    "16. RH_7, Humidity in ironing room, in % \n",
    "17. T8, Temperature in teenager room 2, in Celsius \n",
    "18. RH_8, Humidity in teenager room 2, in % \n",
    "19. T9, Temperature in parents room, in Celsius \n",
    "20. RH_9, Humidity in parents room, in % \n",
    "21. To, Temperature outside (from Chievres weather station), in Celsius \n",
    "22. Pressure (from Chievres weather station), in mm Hg \n",
    "23. RH_out, Humidity outside (from Chievres weather station), in % \n",
    "24. Wind speed (from Chievres weather station), in m/s \n",
    "25. Visibility (from Chievres weather station), in km \n",
    "26. Tdewpoint (from Chievres weather station), Â°C \n",
    "27. rv1, Random variable 1, nondimensional \n",
    "29. rv2, Random variable 2, nondimensional \n",
    "\n",
    "``Output variable (desired target)``:\n",
    "30. Appliances, energy use in Wh\n",
    "\n",
    "\n",
    "We will create a report named `report-all-data.html` in the repo `./analysis`.\n",
    "This report helps us to understand all distribution and correlation in the data set. You can go into that repo and open it in your browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just Random variable for robustness\n",
    "df_all.drop(columns=['date', 'rv1', 'rv2'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get report analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = pdp.ProfileReport(df_all)\n",
    "profile.to_file(outputfile=\"./analysis/report-all-data.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a first ML model to see what kind of information we need to record to (for example) evaluate the capacity of the model, if we suffer from overfitting or underfitting etc. From that we will understand why `mlflow` is a great tool for tracking metrics and save artifacts.\n",
    "\n",
    "If you looked at the report you've seen that we have some categorical features. We encode them first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "target_column = \"Appliances\" # \"y\"\n",
    "\n",
    "# Split data\n",
    "train, test = train_test_split(df_all)\n",
    "\n",
    "train_x = train.drop([target_column], axis=1)\n",
    "test_x = test.drop([target_column], axis=1)\n",
    "train_y = train[target_column]\n",
    "test_y = test[target_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "def eval_metrics(actual, pred):\n",
    "    rmse = np.sqrt(mean_squared_error(actual, pred))\n",
    "    mae = mean_absolute_error(actual, pred)\n",
    "    r2 = r2_score(actual, pred)\n",
    "    return rmse, mae, r2\n",
    "\n",
    "\n",
    "def scatter_plot_result(y_actual, y_pred, model_name):\n",
    "    plt.scatter(y_actual, y_pred)\n",
    "    plt.ylabel('Target predicted')\n",
    "    plt.xlabel('True Target')\n",
    "    plt.title(model_name)\n",
    "    plt.text(500, 250, r'$RMSE=%.2f, R^2$=%.2f, MAE=%.2f' % (np.sqrt(mean_squared_error(y_actual, y_pred)), \n",
    "                                              r2_score(y_actual, y_pred), \n",
    "                                              mean_absolute_error(y_actual, y_pred)))\n",
    "    plt.savefig('./scatter_results-{}.png'.format(model_name))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build our first model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Train model\n",
    "rfp = RandomForestRegressor(random_state=0, n_estimators=100)\n",
    "model = rfp.fit(train_x, train_y)\n",
    "\n",
    "\n",
    "pred_test = model.predict(test_x)\n",
    "# print('rmse: {} - mae: {} - r2: {}'.format(*eval_metrics(test_y, pred_test)))\n",
    "scatter_plot_result(test_y, pred_test, 'RandomForest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> `Retrain your model with another set of parameters and compare results`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a second model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often, we use a second model in order to challenge the first one..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.preprocessing import QuantileTransformer, quantile_transform\n",
    "\n",
    "\n",
    "# Train model\n",
    "lr = ElasticNet(random_state=0, alpha=0.5, l1_ratio=0.2)\n",
    "model = lr.fit(train_x, train_y)\n",
    "\n",
    "\n",
    "pred_test = model.predict(test_x)\n",
    "# print('rmse: {} - mae: {} - r2: {}'.format(*eval_metrics(test_y, pred_test)))\n",
    "scatter_plot_result(test_y, pred_test, 'ElasticNet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> `Retrain your model with another set of parameters and compare results`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point you may want to draw more visualization to compare your models (performance, feature importance, or other metrics..). You understand that we will have do this process EVERY TIME, to compare or analyse any model or ML code. Also, if your data change, your metrics can change. It would be great to have the history of the data ATTACHED to the code's history\n",
    "\n",
    "This were Tracking with MLflow is useful. \n",
    "\n",
    "Same exercise in `train.py`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import TransformedTargetRegressor\n",
    "\n",
    "# If you wish to try on classification problem\n",
    "def log_metrics_classification(y_true, y_prediction):\n",
    "    report = classification_report(y_true, y_prediction, output_dict=True)\n",
    "    for class_ in ['0', '1']:\n",
    "        for metric in report[class_]:\n",
    "            log_name = class_ + '_' + metric\n",
    "            # insert your code here ~ 1 line\n",
    "         \n",
    "        \n",
    "def log_metrics_regression(y_true, y_prediction):\n",
    "    rmse, mae, r2 = eval_metrics(y_true, y_prediction)\n",
    "    # log metrics here ~ 3 lines\n",
    "\n",
    "\n",
    "def run_experiment(df, alpha, l1_ratio):    \n",
    "    # Split data\n",
    "    train, test = train_test_split(df)\n",
    "    \n",
    "    train_x = train.drop([target_column], axis=1)\n",
    "    test_x = test.drop([target_column], axis=1)\n",
    "    train_y = train[target_column]\n",
    "    test_y = test[target_column]\n",
    "\n",
    "    with mlflow.start_run():\n",
    "        print(\"Running with alpha: {} - l1_ratio: {}\".format(alpha, l1_ratio))\n",
    "\n",
    "        # fit models\n",
    "        lr = ElasticNet(random_state=0, alpha=alpha, l1_ratio=l1_ratio)\n",
    "        lr.fit(train_x, train_y)\n",
    "\n",
    "        prediction_test = lr.predict(test_x)\n",
    "\n",
    "        # log parameters\n",
    "        # Your code here ~ 2 lines\n",
    "\n",
    "        # log artifact\n",
    "        scatter_name = './scatter_results-ElasticNet.png'\n",
    "        # save scatter plot as artifact here ~ 2 lines\n",
    "\n",
    "        # log metrics\n",
    "        log_metrics_regression(test_y, prediction_test)\n",
    "\n",
    "        # log sklearn model\n",
    "        # log the sklearn model here  ~ 1 line\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with alpha: 0.1 - l1_ratio: 0.1\n"
     ]
    }
   ],
   "source": [
    "# play yourself with parameters\n",
    "# ! both parameters have min 0 and max 1 ! \n",
    "\n",
    "\n",
    "# Remove break to see all runs\n",
    "for alpha in np.arange(0.1, 1, 0.2):\n",
    "    for l1_ratio in np.arange(0.1, 1, 0.2):\n",
    "        run_experiment(df_all, alpha, l1_ratio)\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
